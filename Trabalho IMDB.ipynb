{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabalho Final da disciplina de Deep Learning\n",
    "## Professor: Luan Porfirio e Silva\n",
    "## Aluno: Recígio Poffo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import das libs necessárias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import IMDB\n",
    "from collections import Counter\n",
    "from string import punctuation\n",
    "\n",
    "import os,glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregando os dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Em um primeiro momento tentei utiliza os dados da lib do IBDM, do torchtext.datasets, mas, por algum motivo, os dados todos eram trazidos com o label de 'positivo'. \n",
    "\n",
    "#### Por isso, optei por baixar os arquivos e carregar os .txt 'a mão'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_iter, test_iter = IMDB(split=('train', 'test'))\n",
    "#reviews = []\n",
    "#labels = []\n",
    "#for (label, line) in test_iter:\n",
    "#    reviews.append(line)\n",
    "#    labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#next(train_iter)\n",
    "reviews = []\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos_folder_path = '.data/IMDB/aclImdb/train/pos'\n",
    "train_neg_folder_path = '.data/IMDB/aclImdb/train/neg'\n",
    "\n",
    "test_pos_folder_path = '.data/IMDB/aclImdb/train/pos'\n",
    "test_neg_folder_path = '.data/IMDB/aclImdb/train/neg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in glob.glob(os.path.join(train_pos_folder_path, '*.txt')):\n",
    "    with open(filename, 'r', encoding=\"utf8\") as f:\n",
    "        text = f.read()\n",
    "        reviews.append(text)\n",
    "        labels.append('pos')\n",
    "\n",
    "for filename in glob.glob(os.path.join(train_neg_folder_path, '*.txt')):\n",
    "    with open(filename, 'r', encoding=\"utf8\") as f:\n",
    "        text = f.read()\n",
    "        reviews.append(text)\n",
    "        labels.append('neg')\n",
    "        \n",
    "for filename in glob.glob(os.path.join(test_pos_folder_path, '*.txt')):\n",
    "    with open(filename, 'r', encoding=\"utf8\") as f:\n",
    "        text = f.read()\n",
    "        reviews.append(text)\n",
    "        labels.append('pos')\n",
    "\n",
    "for filename in glob.glob(os.path.join(test_neg_folder_path, '*.txt')):\n",
    "    with open(filename, 'r', encoding=\"utf8\") as f:\n",
    "        text = f.read()\n",
    "        reviews.append(text)\n",
    "        labels.append('neg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conferência dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 50000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reviews),len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>Bromwell High is a cartoon comedy. It ran at t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>Homelessness (or Houselessness as George Carli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>Brilliant over-acting by Lesley Ann Warren. Be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>This is easily the most underrated film inn th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>This is not the typical Mel Brooks film. It wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neg</th>\n",
       "      <td>Towards the end of the movie, I felt it was to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neg</th>\n",
       "      <td>This is the kind of movie that my enemies cont...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neg</th>\n",
       "      <td>I saw 'Descent' last night at the Stockholm Fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neg</th>\n",
       "      <td>Some films that you pick up for a pound turn o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neg</th>\n",
       "      <td>This is one of the dumbest films, I've ever se...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     0\n",
       "pos  Bromwell High is a cartoon comedy. It ran at t...\n",
       "pos  Homelessness (or Houselessness as George Carli...\n",
       "pos  Brilliant over-acting by Lesley Ann Warren. Be...\n",
       "pos  This is easily the most underrated film inn th...\n",
       "pos  This is not the typical Mel Brooks film. It wa...\n",
       "..                                                 ...\n",
       "neg  Towards the end of the movie, I felt it was to...\n",
       "neg  This is the kind of movie that my enemies cont...\n",
       "neg  I saw 'Descent' last night at the Stockholm Fi...\n",
       "neg  Some films that you pick up for a pound turn o...\n",
       "neg  This is one of the dumbest films, I've ever se...\n",
       "\n",
       "[50000 rows x 1 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = pd.DataFrame(reviews,labels)\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay like most Steven Seagal fans I know I not going expect a masterpiece every time he makes a film but I do expect the film to at least have some sorter budget. The main problem with the copy I watched was the terrible over dubbing I know that in some films this has to be done and I accept that but when they overdub with a totally different actors voice and keep doing this thru out the film it does take the magic of overdubbing away. Also the sets seem to be built with no care as in one scene the sliding glass top in a top secret lab has a massive crack going thru it. I was truly disappointed with this film and only hope Stevens next project will be more finished off before sending the film out for buying/renting. The story of this film had me wondering if I was watching a sci-fi film or not some parts seemed alien like but they never fully explained what was going on I found it very confusing.\n",
      "neg\n"
     ]
    }
   ],
   "source": [
    "print(reviews[22100])\n",
    "print(labels[22100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpando os dados\n",
    "\n",
    "#### Durante os testes reparei que as pontuações e stop words estava fazendo o processo de treinamento ficar mais lento e menos preciso, por isso, removi ambos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removidas pontuações do texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "# Remoção das pontuações do texto\n",
    "print(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, item in enumerate(reviews):\n",
    "    reviews[index] = ''.join([c for c in item.lower() if c not in punctuation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "okay like most steven seagal fans i know i not going expect a masterpiece every time he makes a film but i do expect the film to at least have some sorter budget the main problem with the copy i watched was the terrible over dubbing i know that in some films this has to be done and i accept that but when they overdub with a totally different actors voice and keep doing this thru out the film it does take the magic of overdubbing away also the sets seem to be built with no care as in one scene the sliding glass top in a top secret lab has a massive crack going thru it i was truly disappointed with this film and only hope stevens next project will be more finished off before sending the film out for buyingrenting the story of this film had me wondering if i was watching a scifi film or not some parts seemed alien like but they never fully explained what was going on i found it very confusing\n",
      "neg\n"
     ]
    }
   ],
   "source": [
    "print(reviews[22100])\n",
    "print(labels[22100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removidas stop words do texto utilizando a base do nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Carregadas stop words\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remoção das stowords do texto\n",
    "for index, item in enumerate(reviews):\n",
    "    reviews[index] = ' '.join([c for c in word_tokenize(item) if c not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "okay like steven seagal fans know going expect masterpiece every time makes film expect film least sorter budget main problem copy watched terrible dubbing know films done accept overdub totally different actors voice keep thru film take magic overdubbing away also sets seem built care one scene sliding glass top top secret lab massive crack going thru truly disappointed film hope stevens next project finished sending film buyingrenting story film wondering watching scifi film parts seemed alien like never fully explained going found confusing\n",
      "neg\n"
     ]
    }
   ],
   "source": [
    "print(reviews[22100])\n",
    "print(labels[22100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criação do vocabulário de palavras e conversão do texto para inteiros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Em seguida, utilizando todos os textos, foi criado um dicionário do texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feita junção do texto\n",
    "all_text = ' '.join(reviews)\n",
    "words = all_text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilizado o Counter para contar as palavras\n",
    "count_words = Counter(words)\n",
    "total_words = len(words)\n",
    "sorted_words = count_words.most_common(total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criado vocabulário, utilizando o valor 1 como inicial para o 0 ser utilizado como valor neutro posteriormente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_to_int = {w:i+1 for i, (w,c) in enumerate(sorted_words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'br': 1,\n",
       " 'movie': 2,\n",
       " 'film': 3,\n",
       " 'one': 4,\n",
       " 'like': 5,\n",
       " 'good': 6,\n",
       " 'even': 7,\n",
       " 'would': 8,\n",
       " 'time': 9,\n",
       " 'really': 10,\n",
       " 'story': 11,\n",
       " 'see': 12,\n",
       " 'much': 13,\n",
       " 'well': 14,\n",
       " 'get': 15,\n",
       " 'also': 16,\n",
       " 'people': 17,\n",
       " 'bad': 18,\n",
       " 'great': 19,\n",
       " 'first': 20,\n",
       " 'dont': 21,\n",
       " 'made': 22,\n",
       " 'movies': 23,\n",
       " 'make': 24,\n",
       " 'films': 25,\n",
       " 'could': 26,\n",
       " 'way': 27,\n",
       " 'characters': 28,\n",
       " 'think': 29,\n",
       " 'watch': 30,\n",
       " 'two': 31,\n",
       " 'many': 32,\n",
       " 'seen': 33,\n",
       " 'character': 34,\n",
       " 'never': 35,\n",
       " 'little': 36,\n",
       " 'acting': 37,\n",
       " 'plot': 38,\n",
       " 'best': 39,\n",
       " 'love': 40,\n",
       " 'know': 41,\n",
       " 'life': 42,\n",
       " 'show': 43,\n",
       " 'ever': 44,\n",
       " 'still': 45,\n",
       " 'better': 46,\n",
       " 'end': 47,\n",
       " 'say': 48,\n",
       " 'man': 49,\n",
       " 'scene': 50,\n",
       " 'scenes': 51,\n",
       " 'go': 52,\n",
       " 'something': 53,\n",
       " 'im': 54,\n",
       " 'back': 55,\n",
       " 'doesnt': 56,\n",
       " 'real': 57,\n",
       " 'watching': 58,\n",
       " 'years': 59,\n",
       " 'though': 60,\n",
       " 'thing': 61,\n",
       " 'actors': 62,\n",
       " 'didnt': 63,\n",
       " 'another': 64,\n",
       " 'new': 65,\n",
       " 'actually': 66,\n",
       " 'nothing': 67,\n",
       " 'makes': 68,\n",
       " 'find': 69,\n",
       " 'work': 70,\n",
       " 'funny': 71,\n",
       " 'look': 72,\n",
       " 'old': 73,\n",
       " 'going': 74,\n",
       " 'every': 75,\n",
       " 'lot': 76,\n",
       " 'us': 77,\n",
       " 'part': 78,\n",
       " 'director': 79,\n",
       " 'thats': 80,\n",
       " 'cant': 81,\n",
       " 'quite': 82,\n",
       " 'got': 83,\n",
       " 'cast': 84,\n",
       " 'want': 85,\n",
       " 'pretty': 86,\n",
       " 'things': 87,\n",
       " 'seems': 88,\n",
       " 'young': 89,\n",
       " 'around': 90,\n",
       " 'fact': 91,\n",
       " 'however': 92,\n",
       " 'world': 93,\n",
       " 'take': 94,\n",
       " 'enough': 95,\n",
       " 'give': 96,\n",
       " 'may': 97,\n",
       " 'big': 98,\n",
       " 'ive': 99,\n",
       " 'horror': 100,\n",
       " 'original': 101,\n",
       " 'thought': 102,\n",
       " 'without': 103,\n",
       " 'gets': 104,\n",
       " 'always': 105,\n",
       " 'series': 106,\n",
       " 'right': 107,\n",
       " 'long': 108,\n",
       " 'isnt': 109,\n",
       " 'saw': 110,\n",
       " 'come': 111,\n",
       " 'almost': 112,\n",
       " 'times': 113,\n",
       " 'least': 114,\n",
       " 'theres': 115,\n",
       " 'role': 116,\n",
       " 'point': 117,\n",
       " 'action': 118,\n",
       " 'must': 119,\n",
       " 'interesting': 120,\n",
       " 'whole': 121,\n",
       " 'comedy': 122,\n",
       " 'bit': 123,\n",
       " 'family': 124,\n",
       " 'done': 125,\n",
       " 'music': 126,\n",
       " 'script': 127,\n",
       " 'last': 128,\n",
       " 'anything': 129,\n",
       " 'might': 130,\n",
       " 'hes': 131,\n",
       " 'guy': 132,\n",
       " 'since': 133,\n",
       " 'feel': 134,\n",
       " 'minutes': 135,\n",
       " 'far': 136,\n",
       " 'probably': 137,\n",
       " 'performance': 138,\n",
       " 'kind': 139,\n",
       " 'rather': 140,\n",
       " 'worst': 141,\n",
       " 'yet': 142,\n",
       " 'away': 143,\n",
       " 'sure': 144,\n",
       " 'tv': 145,\n",
       " 'making': 146,\n",
       " 'woman': 147,\n",
       " 'girl': 148,\n",
       " 'found': 149,\n",
       " 'fun': 150,\n",
       " 'played': 151,\n",
       " 'anyone': 152,\n",
       " 'although': 153,\n",
       " 'believe': 154,\n",
       " 'comes': 155,\n",
       " 'trying': 156,\n",
       " 'course': 157,\n",
       " 'especially': 158,\n",
       " 'goes': 159,\n",
       " 'day': 160,\n",
       " 'looks': 161,\n",
       " 'hard': 162,\n",
       " 'shows': 163,\n",
       " 'different': 164,\n",
       " 'put': 165,\n",
       " 'wasnt': 166,\n",
       " 'place': 167,\n",
       " 'maybe': 168,\n",
       " 'book': 169,\n",
       " 'set': 170,\n",
       " 'main': 171,\n",
       " 'reason': 172,\n",
       " 'money': 173,\n",
       " 'worth': 174,\n",
       " 'sense': 175,\n",
       " 'everything': 176,\n",
       " 'looking': 177,\n",
       " 'true': 178,\n",
       " 'ending': 179,\n",
       " 'someone': 180,\n",
       " 'watched': 181,\n",
       " 'plays': 182,\n",
       " '2': 183,\n",
       " 'job': 184,\n",
       " 'actor': 185,\n",
       " 'seem': 186,\n",
       " 'three': 187,\n",
       " 'said': 188,\n",
       " 'takes': 189,\n",
       " 'dvd': 190,\n",
       " 'screen': 191,\n",
       " 'together': 192,\n",
       " 'instead': 193,\n",
       " 'play': 194,\n",
       " 'john': 195,\n",
       " 'beautiful': 196,\n",
       " 'later': 197,\n",
       " '10': 198,\n",
       " 'effects': 199,\n",
       " 'everyone': 200,\n",
       " 'version': 201,\n",
       " 'left': 202,\n",
       " 'seeing': 203,\n",
       " 'special': 204,\n",
       " 'audience': 205,\n",
       " 'night': 206,\n",
       " 'excellent': 207,\n",
       " 'house': 208,\n",
       " 'american': 209,\n",
       " 'idea': 210,\n",
       " 'simply': 211,\n",
       " 'nice': 212,\n",
       " 'wife': 213,\n",
       " 'shot': 214,\n",
       " 'youre': 215,\n",
       " 'read': 216,\n",
       " 'high': 217,\n",
       " 'black': 218,\n",
       " 'less': 219,\n",
       " 'completely': 220,\n",
       " 'second': 221,\n",
       " 'kids': 222,\n",
       " 'help': 223,\n",
       " 'else': 224,\n",
       " 'fan': 225,\n",
       " 'poor': 226,\n",
       " 'star': 227,\n",
       " 'war': 228,\n",
       " 'used': 229,\n",
       " 'given': 230,\n",
       " 'year': 231,\n",
       " 'try': 232,\n",
       " 'father': 233,\n",
       " 'death': 234,\n",
       " 'friends': 235,\n",
       " 'need': 236,\n",
       " 'use': 237,\n",
       " 'rest': 238,\n",
       " 'enjoy': 239,\n",
       " 'home': 240,\n",
       " 'men': 241,\n",
       " 'performances': 242,\n",
       " 'short': 243,\n",
       " 'mind': 244,\n",
       " 'classic': 245,\n",
       " 'either': 246,\n",
       " 'truly': 247,\n",
       " 'along': 248,\n",
       " 'hollywood': 249,\n",
       " 'boring': 250,\n",
       " 'half': 251,\n",
       " 'dead': 252,\n",
       " 'wrong': 253,\n",
       " 'tell': 254,\n",
       " 'production': 255,\n",
       " 'women': 256,\n",
       " 'line': 257,\n",
       " 'remember': 258,\n",
       " 'couple': 259,\n",
       " 'next': 260,\n",
       " 'came': 261,\n",
       " 'recommend': 262,\n",
       " 'start': 263,\n",
       " 'perhaps': 264,\n",
       " 'full': 265,\n",
       " 'let': 266,\n",
       " 'understand': 267,\n",
       " 'wonderful': 268,\n",
       " 'stupid': 269,\n",
       " 'getting': 270,\n",
       " 'others': 271,\n",
       " 'mean': 272,\n",
       " 'moments': 273,\n",
       " 'playing': 274,\n",
       " 'awful': 275,\n",
       " 'keep': 276,\n",
       " 'episode': 277,\n",
       " 'terrible': 278,\n",
       " 'camera': 279,\n",
       " 'small': 280,\n",
       " 'definitely': 281,\n",
       " 'gives': 282,\n",
       " 'often': 283,\n",
       " 'stars': 284,\n",
       " 'sex': 285,\n",
       " 'video': 286,\n",
       " 'early': 287,\n",
       " 'perfect': 288,\n",
       " 'become': 289,\n",
       " 'finally': 290,\n",
       " 'name': 291,\n",
       " 'felt': 292,\n",
       " 'school': 293,\n",
       " 'human': 294,\n",
       " 'face': 295,\n",
       " 'supposed': 296,\n",
       " 'couldnt': 297,\n",
       " 'liked': 298,\n",
       " 'lines': 299,\n",
       " 'dialogue': 300,\n",
       " 'piece': 301,\n",
       " 'person': 302,\n",
       " 'lost': 303,\n",
       " 'absolutely': 304,\n",
       " 'yes': 305,\n",
       " 'top': 306,\n",
       " 'case': 307,\n",
       " 'entire': 308,\n",
       " 'went': 309,\n",
       " 'certainly': 310,\n",
       " 'written': 311,\n",
       " 'live': 312,\n",
       " 'title': 313,\n",
       " 'waste': 314,\n",
       " 'shes': 315,\n",
       " 'sort': 316,\n",
       " 'head': 317,\n",
       " 'budget': 318,\n",
       " 'hope': 319,\n",
       " 'problem': 320,\n",
       " 'style': 321,\n",
       " 'overall': 322,\n",
       " 'several': 323,\n",
       " 'picture': 324,\n",
       " 'loved': 325,\n",
       " 'evil': 326,\n",
       " 'mr': 327,\n",
       " 'worse': 328,\n",
       " 'id': 329,\n",
       " 'becomes': 330,\n",
       " 'fans': 331,\n",
       " '3': 332,\n",
       " 'entertaining': 333,\n",
       " 'cinema': 334,\n",
       " 'boy': 335,\n",
       " 'beginning': 336,\n",
       " 'seemed': 337,\n",
       " 'white': 338,\n",
       " 'already': 339,\n",
       " 'based': 340,\n",
       " 'despite': 341,\n",
       " 'care': 342,\n",
       " 'oh': 343,\n",
       " 'wanted': 344,\n",
       " 'dark': 345,\n",
       " 'example': 346,\n",
       " '\\x96': 347,\n",
       " 'lives': 348,\n",
       " 'guys': 349,\n",
       " 'killer': 350,\n",
       " 'unfortunately': 351,\n",
       " 'mother': 352,\n",
       " 'throughout': 353,\n",
       " 'direction': 354,\n",
       " 'final': 355,\n",
       " 'friend': 356,\n",
       " 'turn': 357,\n",
       " 'totally': 358,\n",
       " '1': 359,\n",
       " 'fine': 360,\n",
       " 'wont': 361,\n",
       " 'wants': 362,\n",
       " 'children': 363,\n",
       " 'amazing': 364,\n",
       " 'sound': 365,\n",
       " 'laugh': 366,\n",
       " 'drama': 367,\n",
       " 'girls': 368,\n",
       " 'guess': 369,\n",
       " 'youll': 370,\n",
       " 'lead': 371,\n",
       " 'tries': 372,\n",
       " 'humor': 373,\n",
       " 'called': 374,\n",
       " 'low': 375,\n",
       " 'writing': 376,\n",
       " 'michael': 377,\n",
       " 'works': 378,\n",
       " 'history': 379,\n",
       " 'turns': 380,\n",
       " 'able': 381,\n",
       " 'enjoyed': 382,\n",
       " 'theyre': 383,\n",
       " 'behind': 384,\n",
       " 'past': 385,\n",
       " 'quality': 386,\n",
       " 'days': 387,\n",
       " 'favorite': 388,\n",
       " 'gave': 389,\n",
       " 'starts': 390,\n",
       " 'son': 391,\n",
       " 'kill': 392,\n",
       " 'game': 393,\n",
       " 'act': 394,\n",
       " 'sometimes': 395,\n",
       " 'side': 396,\n",
       " 'viewer': 397,\n",
       " 'town': 398,\n",
       " 'horrible': 399,\n",
       " 'parts': 400,\n",
       " 'car': 401,\n",
       " 'actress': 402,\n",
       " 'child': 403,\n",
       " 'soon': 404,\n",
       " 'ones': 405,\n",
       " 'expect': 406,\n",
       " 'eyes': 407,\n",
       " 'obviously': 408,\n",
       " 'flick': 409,\n",
       " 'directed': 410,\n",
       " 'thinking': 411,\n",
       " 'art': 412,\n",
       " 'heart': 413,\n",
       " 'brilliant': 414,\n",
       " 'stories': 415,\n",
       " 'ill': 416,\n",
       " 'decent': 417,\n",
       " 'highly': 418,\n",
       " 'run': 419,\n",
       " 'genre': 420,\n",
       " 'feeling': 421,\n",
       " 'late': 422,\n",
       " 'blood': 423,\n",
       " 'stuff': 424,\n",
       " 'fight': 425,\n",
       " 'says': 426,\n",
       " 'close': 427,\n",
       " 'took': 428,\n",
       " 'city': 429,\n",
       " 'except': 430,\n",
       " 'heard': 431,\n",
       " 'hand': 432,\n",
       " 'leave': 433,\n",
       " 'killed': 434,\n",
       " 'kid': 435,\n",
       " 'matter': 436,\n",
       " 'police': 437,\n",
       " 'hell': 438,\n",
       " 'moment': 439,\n",
       " 'wouldnt': 440,\n",
       " 'extremely': 441,\n",
       " 'strong': 442,\n",
       " 'roles': 443,\n",
       " 'happens': 444,\n",
       " 'particularly': 445,\n",
       " 'hour': 446,\n",
       " 'lack': 447,\n",
       " 'involved': 448,\n",
       " 'obvious': 449,\n",
       " 'happened': 450,\n",
       " 'attempt': 451,\n",
       " 'james': 452,\n",
       " 'told': 453,\n",
       " 'chance': 454,\n",
       " 'living': 455,\n",
       " 'violence': 456,\n",
       " 'wonder': 457,\n",
       " 'etc': 458,\n",
       " 'including': 459,\n",
       " 'complete': 460,\n",
       " 'save': 461,\n",
       " 'voice': 462,\n",
       " 'coming': 463,\n",
       " 'murder': 464,\n",
       " 'anyway': 465,\n",
       " 'group': 466,\n",
       " 'daughter': 467,\n",
       " 'age': 468,\n",
       " 'looked': 469,\n",
       " 'please': 470,\n",
       " 'type': 471,\n",
       " 'itbr': 472,\n",
       " 'alone': 473,\n",
       " 'experience': 474,\n",
       " 'god': 475,\n",
       " 'none': 476,\n",
       " 'simple': 477,\n",
       " 'number': 478,\n",
       " 'score': 479,\n",
       " 'exactly': 480,\n",
       " 'slow': 481,\n",
       " 'shown': 482,\n",
       " 'happen': 483,\n",
       " 'ok': 484,\n",
       " 'ago': 485,\n",
       " 'lets': 486,\n",
       " 'interest': 487,\n",
       " 'whose': 488,\n",
       " 'taken': 489,\n",
       " 'brother': 490,\n",
       " 'usually': 491,\n",
       " 'serious': 492,\n",
       " 'david': 493,\n",
       " 'across': 494,\n",
       " 'cinematography': 495,\n",
       " 'stop': 496,\n",
       " 'somewhat': 497,\n",
       " 'annoying': 498,\n",
       " 'hours': 499,\n",
       " 'running': 500,\n",
       " 'sad': 501,\n",
       " 'opening': 502,\n",
       " 'song': 503,\n",
       " 'known': 504,\n",
       " 'ends': 505,\n",
       " 'usual': 506,\n",
       " 'musical': 507,\n",
       " 'possible': 508,\n",
       " 'finds': 509,\n",
       " 'career': 510,\n",
       " 'wish': 511,\n",
       " 'hit': 512,\n",
       " 'released': 513,\n",
       " 'started': 514,\n",
       " 'huge': 515,\n",
       " 'gore': 516,\n",
       " 'seriously': 517,\n",
       " 'relationship': 518,\n",
       " 'scary': 519,\n",
       " 'jokes': 520,\n",
       " 'change': 521,\n",
       " 'saying': 522,\n",
       " 'order': 523,\n",
       " 'crap': 524,\n",
       " 'reality': 525,\n",
       " 'mostly': 526,\n",
       " 'today': 527,\n",
       " 'shots': 528,\n",
       " 'cut': 529,\n",
       " 'ridiculous': 530,\n",
       " 'robert': 531,\n",
       " 'english': 532,\n",
       " 'major': 533,\n",
       " 'taking': 534,\n",
       " 'episodes': 535,\n",
       " 'hilarious': 536,\n",
       " 'cool': 537,\n",
       " 'novel': 538,\n",
       " 'body': 539,\n",
       " 'female': 540,\n",
       " 'talking': 541,\n",
       " '4': 542,\n",
       " 'opinion': 543,\n",
       " 'call': 544,\n",
       " 'apparently': 545,\n",
       " 'directors': 546,\n",
       " 'strange': 547,\n",
       " 'due': 548,\n",
       " '5': 549,\n",
       " 'basically': 550,\n",
       " 'hero': 551,\n",
       " 'important': 552,\n",
       " 'supporting': 553,\n",
       " 'clearly': 554,\n",
       " 'documentary': 555,\n",
       " 'power': 556,\n",
       " 'knows': 557,\n",
       " 'knew': 558,\n",
       " 'events': 559,\n",
       " 'happy': 560,\n",
       " 'view': 561,\n",
       " 'husband': 562,\n",
       " 'turned': 563,\n",
       " 'songs': 564,\n",
       " 'talent': 565,\n",
       " 'king': 566,\n",
       " 'level': 567,\n",
       " 'arent': 568,\n",
       " 'british': 569,\n",
       " 'easily': 570,\n",
       " 'room': 571,\n",
       " 'tells': 572,\n",
       " 'single': 573,\n",
       " 'local': 574,\n",
       " 'rating': 575,\n",
       " 'attention': 576,\n",
       " 'word': 577,\n",
       " 'moviebr': 578,\n",
       " 'bring': 579,\n",
       " 'words': 580,\n",
       " 'problems': 581,\n",
       " 'cheap': 582,\n",
       " 'modern': 583,\n",
       " 'whats': 584,\n",
       " 'silly': 585,\n",
       " 'television': 586,\n",
       " 'beyond': 587,\n",
       " 'sequence': 588,\n",
       " 'whether': 589,\n",
       " 'disappointed': 590,\n",
       " 'jack': 591,\n",
       " 'light': 592,\n",
       " 'falls': 593,\n",
       " 'sets': 594,\n",
       " 'future': 595,\n",
       " 'four': 596,\n",
       " 'five': 597,\n",
       " 'similar': 598,\n",
       " 'paul': 599,\n",
       " 'miss': 600,\n",
       " 'country': 601,\n",
       " 'needs': 602,\n",
       " 'appears': 603,\n",
       " 'giving': 604,\n",
       " 'romantic': 605,\n",
       " 'upon': 606,\n",
       " 'earth': 607,\n",
       " 'comic': 608,\n",
       " 'viewers': 609,\n",
       " 'predictable': 610,\n",
       " 'richard': 611,\n",
       " 'george': 612,\n",
       " 'entertainment': 613,\n",
       " 'talk': 614,\n",
       " 'review': 615,\n",
       " 'within': 616,\n",
       " 'havent': 617,\n",
       " 'feels': 618,\n",
       " 'nearly': 619,\n",
       " 'mention': 620,\n",
       " 'message': 621,\n",
       " 'enjoyable': 622,\n",
       " 'animation': 623,\n",
       " 'bunch': 624,\n",
       " 'filmbr': 625,\n",
       " 'lady': 626,\n",
       " 'theater': 627,\n",
       " 'lots': 628,\n",
       " 'storyline': 629,\n",
       " 'add': 630,\n",
       " 'rock': 631,\n",
       " 'actual': 632,\n",
       " 'using': 633,\n",
       " 'moving': 634,\n",
       " 'points': 635,\n",
       " 'middle': 636,\n",
       " 'surprised': 637,\n",
       " 'named': 638,\n",
       " 'theme': 639,\n",
       " 'mystery': 640,\n",
       " 'among': 641,\n",
       " 'ten': 642,\n",
       " 'dull': 643,\n",
       " 'begins': 644,\n",
       " 'comments': 645,\n",
       " 'writer': 646,\n",
       " 'ways': 647,\n",
       " 'fantastic': 648,\n",
       " 'typical': 649,\n",
       " 'stay': 650,\n",
       " 'showing': 651,\n",
       " 'sequel': 652,\n",
       " 'york': 653,\n",
       " 'elements': 654,\n",
       " 'easy': 655,\n",
       " 'certain': 656,\n",
       " 'thriller': 657,\n",
       " 'team': 658,\n",
       " 'tried': 659,\n",
       " 'clear': 660,\n",
       " 'fall': 661,\n",
       " 'effort': 662,\n",
       " 'near': 663,\n",
       " 'avoid': 664,\n",
       " 'release': 665,\n",
       " 'hate': 666,\n",
       " 'french': 667,\n",
       " 'famous': 668,\n",
       " 'tale': 669,\n",
       " 'parents': 670,\n",
       " 'means': 671,\n",
       " 'sorry': 672,\n",
       " 'somehow': 673,\n",
       " 'peter': 674,\n",
       " 'leads': 675,\n",
       " 'straight': 676,\n",
       " 'kept': 677,\n",
       " 'red': 678,\n",
       " 'buy': 679,\n",
       " 'working': 680,\n",
       " 'greatest': 681,\n",
       " 'dialog': 682,\n",
       " 'doubt': 683,\n",
       " 'form': 684,\n",
       " 'class': 685,\n",
       " 'soundtrack': 686,\n",
       " 'general': 687,\n",
       " 'editing': 688,\n",
       " 'season': 689,\n",
       " 'brought': 690,\n",
       " 'sister': 691,\n",
       " 'weak': 692,\n",
       " 'filmed': 693,\n",
       " 'tom': 694,\n",
       " 'figure': 695,\n",
       " 'feature': 696,\n",
       " 'oscar': 697,\n",
       " 'hear': 698,\n",
       " 'gone': 699,\n",
       " 'whos': 700,\n",
       " 'particular': 701,\n",
       " 'material': 702,\n",
       " 'check': 703,\n",
       " 'viewing': 704,\n",
       " 'learn': 705,\n",
       " 'realistic': 706,\n",
       " 'imagine': 707,\n",
       " 'eventually': 708,\n",
       " 'youve': 709,\n",
       " 'move': 710,\n",
       " 'eye': 711,\n",
       " 'atmosphere': 712,\n",
       " 'fast': 713,\n",
       " 'reviews': 714,\n",
       " 'decided': 715,\n",
       " 'sequences': 716,\n",
       " 'possibly': 717,\n",
       " 'forget': 718,\n",
       " 'period': 719,\n",
       " 'lame': 720,\n",
       " 'deal': 721,\n",
       " 'third': 722,\n",
       " 'premise': 723,\n",
       " 'dance': 724,\n",
       " 'became': 725,\n",
       " 'follow': 726,\n",
       " 'de': 727,\n",
       " 'lee': 728,\n",
       " 'space': 729,\n",
       " 'wait': 730,\n",
       " 'indeed': 731,\n",
       " 'stand': 732,\n",
       " 'japanese': 733,\n",
       " 'sit': 734,\n",
       " 'difficult': 735,\n",
       " 'zombie': 736,\n",
       " 'poorly': 737,\n",
       " 'expected': 738,\n",
       " 'sexual': 739,\n",
       " 'die': 740,\n",
       " 'whatever': 741,\n",
       " 'writers': 742,\n",
       " 'surprise': 743,\n",
       " 'nature': 744,\n",
       " 'crime': 745,\n",
       " 'rent': 746,\n",
       " 'average': 747,\n",
       " '80s': 748,\n",
       " 'suspense': 749,\n",
       " 'leaves': 750,\n",
       " 'okay': 751,\n",
       " 'subject': 752,\n",
       " 'stage': 753,\n",
       " 'killing': 754,\n",
       " 'believable': 755,\n",
       " 'truth': 756,\n",
       " 'screenplay': 757,\n",
       " 'needed': 758,\n",
       " 'filmmakers': 759,\n",
       " 'reading': 760,\n",
       " 'dr': 761,\n",
       " 'meets': 762,\n",
       " 'note': 763,\n",
       " 'meet': 764,\n",
       " 'begin': 765,\n",
       " 'question': 766,\n",
       " 'boys': 767,\n",
       " 'joe': 768,\n",
       " 'romance': 769,\n",
       " 'street': 770,\n",
       " 'realize': 771,\n",
       " 'forced': 772,\n",
       " 'otherwise': 773,\n",
       " 'emotional': 774,\n",
       " 'memorable': 775,\n",
       " 'unless': 776,\n",
       " 'superb': 777,\n",
       " 'older': 778,\n",
       " 'shame': 779,\n",
       " 'write': 780,\n",
       " 'interested': 781,\n",
       " 'minute': 782,\n",
       " 'baby': 783,\n",
       " 'earlier': 784,\n",
       " 'keeps': 785,\n",
       " 'situation': 786,\n",
       " 'weird': 787,\n",
       " 'disney': 788,\n",
       " 'dramatic': 789,\n",
       " 'beauty': 790,\n",
       " 'footage': 791,\n",
       " 'features': 792,\n",
       " 'credits': 793,\n",
       " 'towards': 794,\n",
       " 'ask': 795,\n",
       " 'badly': 796,\n",
       " 'dog': 797,\n",
       " 'total': 798,\n",
       " 'previous': 799,\n",
       " 'hot': 800,\n",
       " 'brings': 801,\n",
       " 'crazy': 802,\n",
       " 'sounds': 803,\n",
       " 'comment': 804,\n",
       " 'male': 805,\n",
       " 'personal': 806,\n",
       " 'plenty': 807,\n",
       " 'worked': 808,\n",
       " 'incredibly': 809,\n",
       " 'society': 810,\n",
       " 'plus': 811,\n",
       " 'directing': 812,\n",
       " 'result': 813,\n",
       " 'admit': 814,\n",
       " 'cheesy': 815,\n",
       " 'quickly': 816,\n",
       " 'perfectly': 817,\n",
       " 'unique': 818,\n",
       " 'deep': 819,\n",
       " 'return': 820,\n",
       " 'america': 821,\n",
       " 'laughs': 822,\n",
       " 'creepy': 823,\n",
       " 'free': 824,\n",
       " 'development': 825,\n",
       " 'leading': 826,\n",
       " 'appear': 827,\n",
       " 'hardly': 828,\n",
       " 'meant': 829,\n",
       " 'brothers': 830,\n",
       " 'open': 831,\n",
       " '20': 832,\n",
       " 'b': 833,\n",
       " 'hands': 834,\n",
       " 'imdb': 835,\n",
       " 'apart': 836,\n",
       " 'casting': 837,\n",
       " 'mark': 838,\n",
       " 'various': 839,\n",
       " 'effect': 840,\n",
       " 'remake': 841,\n",
       " 'create': 842,\n",
       " 'setting': 843,\n",
       " 'christmas': 844,\n",
       " 'bill': 845,\n",
       " 'mess': 846,\n",
       " 'background': 847,\n",
       " 'battle': 848,\n",
       " 'potential': 849,\n",
       " 'scifi': 850,\n",
       " 'forward': 851,\n",
       " 'monster': 852,\n",
       " 'dream': 853,\n",
       " 'powerful': 854,\n",
       " 'portrayed': 855,\n",
       " 'inside': 856,\n",
       " '70s': 857,\n",
       " 'outside': 858,\n",
       " 'fairly': 859,\n",
       " 'business': 860,\n",
       " 'la': 861,\n",
       " 'expecting': 862,\n",
       " 'ideas': 863,\n",
       " 'jane': 864,\n",
       " 'manages': 865,\n",
       " 'fails': 866,\n",
       " 'deserves': 867,\n",
       " 'attempts': 868,\n",
       " 'missing': 869,\n",
       " 'present': 870,\n",
       " 'political': 871,\n",
       " 'secret': 872,\n",
       " 'twist': 873,\n",
       " 'fire': 874,\n",
       " 'dumb': 875,\n",
       " 'fighting': 876,\n",
       " 'unlike': 877,\n",
       " 'fantasy': 878,\n",
       " 'gay': 879,\n",
       " 'pay': 880,\n",
       " 'air': 881,\n",
       " 'ben': 882,\n",
       " 'joke': 883,\n",
       " 'william': 884,\n",
       " 'recently': 885,\n",
       " 'rich': 886,\n",
       " 'front': 887,\n",
       " 'married': 888,\n",
       " 'nudity': 889,\n",
       " 'masterpiece': 890,\n",
       " 'copy': 891,\n",
       " 'reasons': 892,\n",
       " 'match': 893,\n",
       " 'box': 894,\n",
       " 'agree': 895,\n",
       " 'sadly': 896,\n",
       " 'acted': 897,\n",
       " 'break': 898,\n",
       " 'talented': 899,\n",
       " 'plain': 900,\n",
       " 'telling': 901,\n",
       " 'success': 902,\n",
       " 'western': 903,\n",
       " 'cute': 904,\n",
       " 'pure': 905,\n",
       " 'villain': 906,\n",
       " 'incredible': 907,\n",
       " 'missed': 908,\n",
       " 'odd': 909,\n",
       " 'girlfriend': 910,\n",
       " 'doctor': 911,\n",
       " 'caught': 912,\n",
       " 'crew': 913,\n",
       " 'following': 914,\n",
       " 'decides': 915,\n",
       " 'cop': 916,\n",
       " 'social': 917,\n",
       " 'large': 918,\n",
       " 'considering': 919,\n",
       " 'waiting': 920,\n",
       " 'mentioned': 921,\n",
       " 'sees': 922,\n",
       " 'members': 923,\n",
       " 'uses': 924,\n",
       " 'flat': 925,\n",
       " 'popular': 926,\n",
       " 'ended': 927,\n",
       " 'hold': 928,\n",
       " 'slightly': 929,\n",
       " 'suddenly': 930,\n",
       " 'public': 931,\n",
       " 'wasted': 932,\n",
       " 'pace': 933,\n",
       " 'compared': 934,\n",
       " 'neither': 935,\n",
       " 'sweet': 936,\n",
       " 'wrote': 937,\n",
       " 'spent': 938,\n",
       " 'entirely': 939,\n",
       " 'kills': 940,\n",
       " 'rate': 941,\n",
       " 'created': 942,\n",
       " 'intelligent': 943,\n",
       " 'familiar': 944,\n",
       " 'office': 945,\n",
       " 'audiences': 946,\n",
       " 'cause': 947,\n",
       " 'ultimately': 948,\n",
       " 'tension': 949,\n",
       " 'era': 950,\n",
       " 'scott': 951,\n",
       " 'bored': 952,\n",
       " 'convincing': 953,\n",
       " 'clever': 954,\n",
       " 'visual': 955,\n",
       " 'escape': 956,\n",
       " 'party': 957,\n",
       " 'moves': 958,\n",
       " 'cartoon': 959,\n",
       " 'basic': 960,\n",
       " 'credit': 961,\n",
       " 'biggest': 962,\n",
       " 'mary': 963,\n",
       " 'list': 964,\n",
       " 'revenge': 965,\n",
       " 'consider': 966,\n",
       " 'fear': 967,\n",
       " 'laughing': 968,\n",
       " 'recent': 969,\n",
       " 'successful': 970,\n",
       " 'spirit': 971,\n",
       " 'island': 972,\n",
       " 'spend': 973,\n",
       " 'trouble': 974,\n",
       " 'positive': 975,\n",
       " 'violent': 976,\n",
       " 'gun': 977,\n",
       " 'choice': 978,\n",
       " 'cover': 979,\n",
       " 'dancing': 980,\n",
       " 'appreciate': 981,\n",
       " 'books': 982,\n",
       " 'former': 983,\n",
       " 'died': 984,\n",
       " 'cold': 985,\n",
       " 'water': 986,\n",
       " 'speak': 987,\n",
       " 'science': 988,\n",
       " 'zombies': 989,\n",
       " 'filled': 990,\n",
       " 'singing': 991,\n",
       " 'concept': 992,\n",
       " '12': 993,\n",
       " 'portrayal': 994,\n",
       " 'younger': 995,\n",
       " 'produced': 996,\n",
       " 'solid': 997,\n",
       " 'value': 998,\n",
       " 'adult': 999,\n",
       " '8': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_to_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertendo vocabulário para inteiros\n",
    "reviews_int = []\n",
    "for review in reviews:\n",
    "    r = [vocab_to_int[w] for w in review.split()]\n",
    "    reviews_int.append(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fazendo o encode das labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_labels = [1 if label =='pos' else 0 for label in labels]\n",
    "encoded_labels = np.array(encoded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analisando os dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vendo alguns tutoriais na internet e lembrando das outras diciplinas, removi os reviews vazios e aqueles que possuiam mais de 300 palavras. Isso facilita o treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "reviews_len = [len(x) for x in reviews_int]\n",
    "pd.Series(reviews_len).hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    50000.000000\n",
       "mean       124.238000\n",
       "std         93.610184\n",
       "min          4.000000\n",
       "25%         66.000000\n",
       "50%         92.000000\n",
       "75%        151.000000\n",
       "max       1449.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(reviews_len).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removendo os reviews muito grandes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_int = [ reviews_int[i] for i, l in enumerate(reviews_len) if l>0 ]\n",
    "encoded_labels = [ encoded_labels[i] for i, l in enumerate(reviews_len) if l> 0 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_len = [len(x) for x in reviews_int]\n",
    "reviews_int = [ reviews_int[i] for i, l in enumerate(reviews_len) if l<301 ]\n",
    "encoded_labels = [ encoded_labels[i] for i, l in enumerate(reviews_len) if l<301 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.Series(encoded_labels).hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    47022.000000\n",
       "mean         0.496193\n",
       "std          0.499991\n",
       "min          0.000000\n",
       "25%          0.000000\n",
       "50%          0.000000\n",
       "75%          1.000000\n",
       "max          1.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(encoded_labels).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Em seguida, adicionado número de valor 0 em todos os exemplos até eles terem o tamanho padrão de 300."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_features(reviews_int, seq_length):\n",
    "    \n",
    "    ##Return features of review_ints, where each review is padded with 0's or truncated to the input seq_length.\n",
    "    \n",
    "    features = np.zeros((len(reviews_int), seq_length), dtype = int)\n",
    "    \n",
    "    for i, review in enumerate(reviews_int):\n",
    "        review_len = len(review)\n",
    "        \n",
    "        if review_len <= seq_length:\n",
    "            zeroes = list(np.zeros(seq_length-review_len))\n",
    "            new = zeroes+review        \n",
    "        elif review_len > seq_length:\n",
    "            new = review[0:seq_length]\n",
    "        \n",
    "        features[i,:] = np.array(new)\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47022"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reviews_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pad_features(reviews_int,300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47022"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Como não funcionou como eu esperava a leitura do IMDB() do próprio torchtext, seguindo uma idéia da internet separei manualmente os textos em treinamento, validação e teste na fração de 80/10/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_feat = len(features)\n",
    "split_frac = 0.8\n",
    "\n",
    "train_x = np.array(features[0:int(split_frac*len_feat)])\n",
    "train_y = np.array(encoded_labels[0:int(split_frac*len_feat)])\n",
    "\n",
    "remaining_x = np.array(features[int(split_frac*len_feat):])\n",
    "remaining_y = np.array(encoded_labels[int(split_frac*len_feat):])\n",
    "\n",
    "valid_x = np.array(remaining_x[0:int(len(remaining_x)*0.5)])\n",
    "valid_y = np.array(remaining_y[0:int(len(remaining_y)*0.5)])\n",
    "\n",
    "test_x = np.array(remaining_x[int(len(remaining_x)*0.5):])\n",
    "test_y = np.array(remaining_y[int(len(remaining_y)*0.5):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0, ...,  8751,  2164,   109],\n",
       "       [    0,     0,     0, ...,  5759,   223,   271],\n",
       "       [    0,     0,     0, ...,    12,    82,   283],\n",
       "       ...,\n",
       "       [    0,     0,     0, ...,   476,    44,   450],\n",
       "       [    0,     0,     0, ...,   416,   732, 41280],\n",
       "       [    0,     0,     0, ...,    46,    78,    42]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utilizando o DataLoader apra carregar os dados em batch no tamanho de 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Tensor datasets\n",
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "valid_data = TensorDataset(torch.from_numpy(valid_x), torch.from_numpy(valid_y))\n",
    "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
    "\n",
    "# dataloaders\n",
    "batch_size = 50\n",
    "\n",
    "# make sure to SHUFFLE your data\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input size:  torch.Size([50, 300])\n",
      "Sample input: \n",
      " tensor([[    0,     0,     0,  ...,   833,    23,   542],\n",
      "        [    0,     0,     0,  ...,  8038,  2362, 17197],\n",
      "        [    0,     0,     0,  ...,  8313,   180,   317],\n",
      "        ...,\n",
      "        [    0,     0,     0,  ..., 15543, 14221,   245],\n",
      "        [    0,     0,     0,  ...,  5719,     6,   723],\n",
      "        [    0,     0,     0,  ...,    98,   728,   225]], dtype=torch.int32)\n",
      "\n",
      "Sample label size:  torch.Size([50])\n",
      "Sample label: \n",
      " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# obtain one batch of training data\n",
    "dataiter = iter(valid_loader)\n",
    "sample_x, sample_y = dataiter.next()\n",
    "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
    "print('Sample input: \\n', sample_x)\n",
    "print()\n",
    "print('Sample label size: ', sample_y.size()) # batch_size\n",
    "print('Sample label: \\n', sample_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "train_on_gpu = False\n",
    "\n",
    "class AnaliseIMDBLSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
    "       \n",
    "        super().__init__()\n",
    "\n",
    "        # variaveis\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # LSTM\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        # dropout\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        # sigmoid \n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        \n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "    \n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "\n",
    "        sig_out = self.sig(out)\n",
    "        \n",
    "        # reshape dos dados\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "        sig_out = sig_out[:, -1]\n",
    "        \n",
    "        return sig_out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "       \n",
    "        # inicia os dados da camada hidden\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnaliseIMDBLSTM(\n",
      "  (embedding): Embedding(121079, 256)\n",
      "  (lstm): LSTM(256, 128, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
      "  (sig): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Configuração da Rede\n",
    "vocab_size = len(vocab_to_int)+1 # +1 for the 0 padding\n",
    "output_size = 1\n",
    "embedding_dim = 256\n",
    "hidden_dim = 128\n",
    "n_layers = 2\n",
    "net = AnaliseIMDBLSTM(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execução do treinamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eu tive um problema com o tamanho dos batchs. Como eles não eram exatos, os ultimos sempre ficavam menores e travavam o treinamento. Por isso, tanto no loop de teste, quando no de validação, eu testo o tamanho. Depois, em conversa com colegas, descobri que tem um parâmetro para tratar isso, contudo, acabei deixando assim mesmo, me ensinou várias coisas o erro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....................................................................................................\n",
      "Epoch: 1/4... Step: 100... Loss: 0.461339... Val Loss: 0.951206\n",
      "....................................................................................................\n",
      "Epoch: 1/4... Step: 200... Loss: 0.438513... Val Loss: 0.527502\n",
      "....................................................................................................\n",
      "Epoch: 1/4... Step: 300... Loss: 0.427189... Val Loss: 0.785896\n",
      "....................................................................................................\n",
      "Epoch: 1/4... Step: 400... Loss: 0.477046... Val Loss: 0.790260\n",
      "....................................................................................................\n",
      "Epoch: 1/4... Step: 500... Loss: 0.380977... Val Loss: 0.503211\n",
      "....................................................................................................\n",
      "Epoch: 1/4... Step: 600... Loss: 0.442282... Val Loss: 0.612322\n",
      "....................................................................................................\n",
      "Epoch: 1/4... Step: 700... Loss: 0.349994... Val Loss: 0.218859\n",
      "...................................................................................................\n",
      "Epoch: 2/4... Step: 800... Loss: 0.365741... Val Loss: 0.394033\n",
      "....................................................................................................\n",
      "Epoch: 2/4... Step: 900... Loss: 0.262664... Val Loss: 0.257618\n",
      "....................................................................................................\n",
      "Epoch: 2/4... Step: 1000... Loss: 0.230955... Val Loss: 0.233437\n",
      "....................................................................................................\n",
      "Epoch: 2/4... Step: 1100... Loss: 0.292993... Val Loss: 0.366604\n",
      "....................................................................................................\n",
      "Epoch: 2/4... Step: 1200... Loss: 0.360618... Val Loss: 0.279228\n",
      "....................................................................................................\n",
      "Epoch: 2/4... Step: 1300... Loss: 0.319006... Val Loss: 0.198086\n",
      "....................................................................................................\n",
      "Epoch: 2/4... Step: 1400... Loss: 0.197754... Val Loss: 0.173291\n",
      "....................................................................................................\n",
      "Epoch: 2/4... Step: 1500... Loss: 0.072893... Val Loss: 0.201978\n",
      "...................................................................................................\n",
      "Epoch: 3/4... Step: 1600... Loss: 0.175391... Val Loss: 0.132364\n",
      "....................................................................................................\n",
      "Epoch: 3/4... Step: 1700... Loss: 0.035402... Val Loss: 0.128399\n",
      "....................................................................................................\n",
      "Epoch: 3/4... Step: 1800... Loss: 0.144184... Val Loss: 0.154495\n",
      "....................................................................................................\n",
      "Epoch: 3/4... Step: 1900... Loss: 0.114168... Val Loss: 0.219030\n",
      "....................................................................................................\n",
      "Epoch: 3/4... Step: 2000... Loss: 0.054021... Val Loss: 0.142183\n",
      "....................................................................................................\n",
      "Epoch: 3/4... Step: 2100... Loss: 0.023462... Val Loss: 0.143226\n",
      "....................................................................................................\n",
      "Epoch: 3/4... Step: 2200... Loss: 0.032030... Val Loss: 0.076696\n",
      "...................................................................................................\n",
      "Epoch: 4/4... Step: 2300... Loss: 0.092879... Val Loss: 0.084303\n",
      "....................................................................................................\n",
      "Epoch: 4/4... Step: 2400... Loss: 0.024256... Val Loss: 0.075010\n",
      "....................................................................................................\n",
      "Epoch: 4/4... Step: 2500... Loss: 0.020085... Val Loss: 0.038713\n",
      "....................................................................................................\n",
      "Epoch: 4/4... Step: 2600... Loss: 0.010896... Val Loss: 0.142971\n",
      "....................................................................................................\n",
      "Epoch: 4/4... Step: 2700... Loss: 0.117812... Val Loss: 0.060444\n",
      "....................................................................................................\n",
      "Epoch: 4/4... Step: 2800... Loss: 0.011948... Val Loss: 0.046713\n",
      "....................................................................................................\n",
      "Epoch: 4/4... Step: 2900... Loss: 0.026527... Val Loss: 0.073887\n",
      "....................................................................................................\n",
      "Epoch: 4/4... Step: 3000... Loss: 0.005868... Val Loss: 0.047152\n",
      "..........."
     ]
    }
   ],
   "source": [
    "# parametro para usar GPU\n",
    "train_on_gpu = False\n",
    "\n",
    "# loss e otimização\n",
    "lr=0.001\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "# número de épocas\n",
    "epochs = 4\n",
    "\n",
    "# contador para saida\n",
    "counter = 0\n",
    "print_every = 100\n",
    "\n",
    "# seguindo alguns tutorias na internet, adicionado um clip de gradiante para melhorar resultado\n",
    "clip=5\n",
    "\n",
    "if(train_on_gpu):\n",
    "    net.cuda()\n",
    "\n",
    "net.train()\n",
    "for e in range(epochs):\n",
    "    \n",
    "    # inicializa camada hidden\n",
    "    h = net.init_hidden(batch_size)\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        counter += 1\n",
    "\n",
    "        if(train_on_gpu):\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "        # confere se o batch tem o tamanho correto\n",
    "        if(len(inputs) ==batch_size ):\n",
    "            \n",
    "            print('.',end='')\n",
    "                \n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            # zera gradientes\n",
    "            net.zero_grad()\n",
    "\n",
    "            # pega resultados\n",
    "            inputs = inputs.type(torch.LongTensor)\n",
    "            output, h = net(inputs, h)\n",
    "\n",
    "            # calcula a loss e faz o backward\n",
    "            loss = criterion(output.squeeze(), labels.float())\n",
    "            loss.backward()\n",
    "            \n",
    "            # dica de tutorial da internet para resolver bug/problema que pode ocorrer de clip de gradiante\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            optimizer.step()\n",
    "\n",
    "        # gera resultado intermediário de loss\n",
    "        if counter % print_every == 0:\n",
    "            \n",
    "            val_h = net.init_hidden(batch_size)\n",
    "            val_losses = []\n",
    "            net.eval()\n",
    "            for inputs, labels in valid_loader:\n",
    "\n",
    "                # confere tamanho do batch\n",
    "                if(len(inputs)==batch_size):\n",
    "                    \n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "                    if(train_on_gpu):\n",
    "                        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "                    # calcula loss do dataset de validação\n",
    "                    inputs = inputs.type(torch.LongTensor)\n",
    "                    output, val_h = net(inputs, val_h)\n",
    "                    val_loss = criterion(output.squeeze(), labels.float())\n",
    "\n",
    "                    val_losses.append(val_loss.item())\n",
    "\n",
    "            net.train()\n",
    "            print('')\n",
    "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teste na massa de teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..............................................................................................Test loss: 0.029\n",
      "Test accuracy: 0.991\n"
     ]
    }
   ],
   "source": [
    "# variavel de gpu\n",
    "train_on_gpu = False\n",
    "\n",
    "test_losses = [] #\n",
    "num_correct = 0\n",
    "\n",
    "h = net.init_hidden(batch_size)\n",
    "\n",
    "net.eval()\n",
    "\n",
    "# avaliação dos dados de teste\n",
    "for inputs, labels in test_loader:\n",
    "\n",
    "    if(len(inputs) ==batch_size ):\n",
    "            \n",
    "        print('.',end='')\n",
    "        h = tuple([each.data for each in h])\n",
    "\n",
    "        # pega resultados\n",
    "        inputs = inputs.type(torch.LongTensor)\n",
    "        output, h = net(inputs, h)\n",
    "\n",
    "        # calcula loss\n",
    "        test_loss = criterion(output.squeeze(), labels.float())\n",
    "        test_losses.append(test_loss.item())\n",
    "\n",
    "        pred = torch.round(output.squeeze())\n",
    "\n",
    "        # compara resultados\n",
    "        correct_tensor = pred.eq(labels.float().view_as(pred))\n",
    "        correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "        num_correct += np.sum(correct)\n",
    "\n",
    "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "\n",
    "test_acc = num_correct/len(test_loader.dataset)\n",
    "print(\"Test accuracy: {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4659"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Os resultados foram bastante bons. Inicialmente, eu havia utilizado uma configuração de rede menor:\n",
    "\n",
    "##### output_size = 1,  embedding_dim = 128, hidden_dim = 64, n_layers = 2\n",
    "\n",
    "#### Os resultados haviam sido um pouco pior, então eu aumentei um pouco ela. Também notei melhora removendo os stop words. Também, não sei se deveria usar todo o conjunto de dados e divi-los em treinamento/validação/teste, mas não me pareceu errado. Aparentemente a base do IMDB é dividida 50/50% nos seus dados de treinamento e teste, mas ignorei isso e carreguei tudo. \n",
    "\n",
    "#### Como melhoria, seria possível tambem remover as palavras com menores ocorrências, cheguei a começar a fazer isso mas não terminei e não inclui no notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Referências: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://colab.research.google.com/github/pytorch/text/blob/master/examples/legacy_tutorial/migration_tutorial.ipynb#scrollTo=iTotRtXe1CWn\n",
    "# https://towardsdatascience.com/sentiment-analysis-using-lstm-step-by-step-50d074f09948\n",
    "# https://github.com/biplob004/codeShare/blob/main/lstm-sentiment-analysis.ipynb\n",
    "# https://towardsdatascience.com/multiclass-text-classification-using-lstm-in-pytorch-eac56baed8df\n",
    "# https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
